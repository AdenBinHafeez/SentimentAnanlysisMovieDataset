{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis - Cornell Moview Review Dataset\n",
        "\n",
        "The dataset (polarity v2) used in this example can be found at http://www.cs.cornell.edu/people/pabo/movie-review-data/"
      ],
      "metadata": {
        "id": "I4kTgQ3Nj7wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load training dataset"
      ],
      "metadata": {
        "id": "xvsKPxIJj7wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from numpy import *\n",
        "import matplotlib.pyplot as plt\n",
        "X_data = loadtxt(\"data/X_train.txt\") \n",
        "print X_data.shape\n",
        "X = X_data"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 4840)\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "ETl4fLMfj7wV",
        "outputId": "b1e24f67-ba7b-4e01-a489-456dd1ae5861"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2000 training examples and 4840 features for each row of the training examples."
      ],
      "metadata": {
        "id": "AC72RMnGj7wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = loadtxt(\"data/cornell/y_train.txt\", dtype = int) \n",
        "print y_data.shape\n",
        "y = y_data"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "Xp3oMZefj7wW",
        "outputId": "0b742df7-9506-4573-bfe1-25655e6633fe"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `y_data` is used to train and test the learning model and indicates a 0 or 1 value, 0=negative review and 1=positive review. The length of 2000 for y corresponds to the training label for each row in X_data.\n",
        "\n",
        "The dataset consists of 1000 negative examples and 1000 positive examples of movie reviews. In the raw data, the first 1000 feature vectors correspond to positive examples and the last 1000 correspond to negative examples. Before training the model, we need to shuffle the data and then slice 1000 training rows and 1000 test rows."
      ],
      "metadata": {
        "id": "yClUfGeGj7wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_new, y_new = shuffle(X, y)\n",
        "\n",
        "X_train = X_new[:1000]\n",
        "y_train = y_new[:1000]\n",
        "X_test = X_new[1000:]\n",
        "y_test = y_new[1000:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "gkGK_LuNj7wW"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Grui-Kwzj7wX"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model\n",
        "Fit the logistic regression model to 1000 rows of the training data and test the accuracy by predicting on the training data that was used to build the model. The lower the accuracy is on the training set, may be an indication of over fitting."
      ],
      "metadata": {
        "id": "i1tK_356j7wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "y_pred_train = logreg.predict(X_train)\n",
        "print \"Accuracy on training set:\", logreg.score(X_train, y_train)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 0.997\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "q9f3S6P0j7wX",
        "outputId": "94dcd1da-ed49-49c0-f1d6-19be6a4a9c82"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the model\n",
        "\n",
        "Test the learning model on the remaining 1000 test rows that were not used to train the model."
      ],
      "metadata": {
        "id": "gsAIDwzjj7wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = logreg.predict(X_test)\n",
        "print \"Accuracy on test set:\", logreg.score(X_test, y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.905\n"
          ],
          "name": "stdout"
        }
      ],
      "metadata": {
        "id": "oRiFKDjfj7wY",
        "outputId": "f7e93578-e0d1-4791-9291-001be32395bc"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This accuracy would be at or near state of the art, but would need additional scruntiny from others to ensure that I didn't mess something up. :)"
      ],
      "metadata": {
        "id": "JBRV-X-qj7wZ"
      }
    }
  ]
}